{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOMIXTRxkACyhPGC9YVBo+y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/viett887g/VSM_20230311/blob/main/SVM_20230311.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dBZcAk_CMl3e",
        "outputId": "69fdc837-604a-40e5-e4c4-9d0c513bbe74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "from sklearn import preprocessing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "o-tI0FQpMm7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **I. Chọn dataset :USA Housing**"
      ],
      "metadata": {
        "id": "v-6_qaHGTsYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def readdata(file):\n",
        "    data = pd.read_csv(file)\n",
        "    del data['Address']\n",
        "    data = np.array(data)\n",
        "    data = preprocessing.MinMaxScaler().fit_transform(data)\n",
        "    X = data[:,:-1]\n",
        "    y = data[:, -1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "    return X_train, X_test, y_train, y_test"
      ],
      "metadata": {
        "id": "NsZcbM5aT2PT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_cost(theta, X, y, C):\n",
        "    m = X.shape[0]\n",
        "    hinge_loss = 1 - y * np.dot(X, theta)\n",
        "    hinge_loss[hinge_loss < 0] = 0\n",
        "    cost = 0.5 * np.dot(theta, theta) + (C * np.sum(hinge_loss)) / m\n",
        "    return cost\n",
        "\n",
        "def svm_gradient(theta, X, y, C):\n",
        "    m = X.shape[0]\n",
        "    pred = np.dot(X, theta)\n",
        "    pred[pred >= 0] = 1\n",
        "    pred[pred < 0] = -1\n",
        "    gradient = theta - (C * np.dot(X.T, y * pred)) / m\n",
        "    return gradient"
      ],
      "metadata": {
        "id": "1NBgkgdBT8gf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# II. Chọn 2 phương pháp **Gradient Descent (GD)**, **Momentum Based GD** tối ưu để tối ưu hoá loss **SVM** cho tập dữ liệu USA Housing"
      ],
      "metadata": {
        "id": "mNH8uAIKTxzv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.1 Gradient Descent (GD)**"
      ],
      "metadata": {
        "id": "nij7ICY7UFDv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def svm_train(X, y, C, learning_rate, num_iterations):\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros(n)\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        cost = svm_cost(theta, X, y, C)\n",
        "        gradient = svm_gradient(theta, X, y, C)\n",
        "        theta -= learning_rate * gradient\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            # print(\"Iteration:\", i, \"Cost:\", cost)\n",
        "            pass"
      ],
      "metadata": {
        "id": "u5_3X_qTT-lB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **2.2 Momentum Based GD**"
      ],
      "metadata": {
        "id": "dqhqQUPkUHk2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E55cXG3pMbAQ"
      },
      "outputs": [],
      "source": [
        "def svm_train_momentum(X, y, C, learning_rate, num_iterations, momentum):\n",
        "    m, n = X.shape\n",
        "    theta = np.zeros(n)\n",
        "    velocity = np.zeros(n)\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        cost = svm_cost(theta, X, y, C)\n",
        "        gradient = svm_gradient(theta, X, y, C)\n",
        "\n",
        "        velocity = momentum * velocity + learning_rate * gradient\n",
        "        theta -= velocity\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            # print(\"Iteration:\", i, \"Cost:\", cost)\n",
        "            pass\n",
        "\n",
        "    return theta"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **III. So sánh hiệu suất của các thuật toán**"
      ],
      "metadata": {
        "id": "VB96sQ9eUKsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_performance(X_test, y_test, w):\n",
        "    # Tính toán dự đoán trên tập test\n",
        "    y_pred = np.dot(X_test, w)\n",
        "\n",
        "    # Tính toán Mean Squared Error (MSE)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "    # Tính toán Rmse\n",
        "    rmse = np.sqrt(mse)\n",
        "\n",
        "\n",
        "    # Trả về kết quả hiệu suất của mô hình\n",
        "    return mse, rmse"
      ],
      "metadata": {
        "id": "cBL4SC4_TG0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    # Đọc dữ liệu từ file\n",
        "    X_train, X_test, y_train, y_test = readdata(\"/content/drive/MyDrive/USA_Housing.csv\")\n",
        "\n",
        "    # Khởi tạo các tham số huấn luyện\n",
        "    C = 1.0\n",
        "    learning_rate = 0.01\n",
        "    num_iterations = 1000\n",
        "    momentum = 0.9\n",
        "    # Huấn luyện mô hình SVM\n",
        "    theta = svm_train(X_train, y_train, C, learning_rate, num_iterations)\n",
        "    mse, rmse=evaluate_performance(X_test, y_test, theta)\n",
        "    # Tính giá trị loss trên tập test\n",
        "    test_cost = svm_cost(theta, X_test, y_test, C)\n",
        "    print(\"Giá trị loss tối ưu trên tập test của thuật toán gradient descent :\", test_cost)\n",
        "    print(\"Mean Squared Error của Gradient Descents (MSE):\", mse)\n",
        "    print('Root Mean Squared Error của Gradient Descents:',rmse)\n",
        "    print('\\n','\\n')\n",
        "\n",
        "    theta=svm_train_momentum(X_train, y_train, C, learning_rate, num_iterations, momentum)\n",
        "    test_cost = svm_cost(theta, X_test, y_test, C)\n",
        "    print(\"Giá trị loss tối ưu trên tập test của thuật toán Momentum Based Gradient Descent :\", test_cost)\n",
        "    mse, rmse=evaluate_performance(X_test, y_test, theta)\n",
        "    print(\"Mean Squared Error (MSE) của momentum_based_gradient_descent:\", mse)\n",
        "    print('Root Mean Squared Error của momentum_based_gradient_descent:',rmse)\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3qp6Db_MhVv",
        "outputId": "badbe20d-a9e5-4e12-a814-81325551d21a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Giá trị loss tối ưu trên tập test của thuật toán gradient descent : 0.8302422295015303\n",
            "Mean Squared Error của Gradient Descents (MSE): 0.03207110809670577\n",
            "Root Mean Squared Error của Gradient Descents: 0.1790840810812222\n",
            "\n",
            " \n",
            "\n",
            "Giá trị loss tối ưu trên tập test của thuật toán Momentum Based Gradient Descent : 0.8302420371372262\n",
            "Mean Squared Error (MSE) của momentum_based_gradient_descent: 0.0320794725113606\n",
            "Root Mean Squared Error của momentum_based_gradient_descent: 0.17910743287580388\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **IV.Nhận xét và đánh giá**\n",
        "\n",
        "**Nhận xét:**\n",
        "\n",
        "giá trị loss tối ưu trên tập test của cả hai thuật toán không có sự khác biệt lớn (0.8302422295015303 cho GD và 0.8302420371372262 cho Momentum GD). Tuy nhiên, MSE và RMSE của Momentum GD (0.0320794725113606 và 0.17910743287580388) có một ít lớn hơn so với MSE và RMSE của GD (0.03207110809670577 và 0.1790840810812222).\n",
        "\n",
        "**Kết luận :**\n",
        "\n",
        "Dựa trên kết quả này, không có sự khác biệt rõ ràng giữa hai thuật toán trong trường hợp cụ thể này. Tuy nhiên, Momentum GD có thể giúp giảm sự dao động và tăng tính ổn định trong quá trình tối ưu hóa."
      ],
      "metadata": {
        "id": "JchbWfvyUf15"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **V. Ưu nhược điểm của phương pháp tối ưu.**"
      ],
      "metadata": {
        "id": "tNtCqQHfVIVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ưu điểm của phương pháp tối ưu Gradient Descent (GD):**\n",
        "\n",
        "Đơn giản và dễ hiểu: Gradient Descent là một phương pháp tối ưu đơn giản và dễ hiểu. Nó chỉ yêu cầu tính đạo hàm đơn giản của hàm mất mát, và cập nhật các tham số theo hướng giảm độ dốc của đạo hàm.\n",
        "\n",
        "Hiệu quả tính toán: GD thường được sử dụng trong nhiều mô hình học máy công nghệ cao, bởi vì nó có thể xử lý tập dữ liệu lớn một cách hiệu quả. Việc tính toán gradient chỉ phụ thuộc vào kích thước của batch, và không yêu cầu tính toán ma trận đầy đủ.\n",
        "\n",
        "Hội tụ tới điểm tối ưu cục bộ: Nếu được cài đặt đúng, GD có khả năng hội tụ tới điểm tối ưu cục bộ. Nếu hàm mất mát là lồi và tốt hơn, GD có thể cung cấp giải pháp tối ưu đối tượng.\n",
        "\n",
        "**Nhược điểm của phương pháp tối ưu Gradient Descent (GD):**\n",
        "\n",
        "Tốc độ hội tụ chậm: GD có thể hội tụ chậm đối với một số bài toán, đặc biệt là khi hàm mất mát có đặc trưng vùng trong lỏng lẻo hoặc bề mặt lõm. Việc cập nhật các tham số trên toàn bộ dữ liệu có thể cần nhiều vòng lặp để đạt điểm tối ưu.\n",
        "Momentum Based Gradient Descent (GD với đà):\n"
      ],
      "metadata": {
        "id": "D-xdOT5SVGsJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ưu điểm Momentum Based GD** có thể giúp cải thiện tốc độ hội tụ của GD. Bằng cách tích lũy đà từ các bước trước, nó có khả năng vượt qua các mục tiêu hẹp và giảm thiểu trỗi dạng cục bộ, đồng thời giảm ảnh hưởng của các điểm nhiễu trên đường đi hội tụ.\n",
        "\n",
        "**Nhược điểm Momentum Based GD**: Mặc dù hiệu quả hơn so với GD cơ bản, tuy nhiên không phải lúc nào Momentum Based GD cũng tốt hơn. Trong một số trường hợp, điều này có thể gây ra quá đà, kéo dài thời gian hội tụ hoặc không hội tụ tới điểm tối ưu. Bên cạnh đó, việc điều chỉnh thông số đà có thể không dễ dàng và yêu cầu thử nghiệm."
      ],
      "metadata": {
        "id": "ayQ22GBuWcHN"
      }
    }
  ]
}